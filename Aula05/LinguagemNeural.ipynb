{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# <p align=\"center\"><span style=\"color:green;\">Processamento de Linguagem Natural (PLN)</span></p>      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <p1><span style=\"color: rgba(144, 238, 144, 0.5); \">(Perspectiva de Aprendizado de M√°quina)</span></p1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pr√©-processamento de Texto  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üë®üèª‚Äçüè´ **Pr√©-processamento de Dados em Aprendizado de M√°quina**\n",
    "\n",
    "* O pr√©-processamento de dados √© uma etapa crucial para qualquer algoritmo de Aprendizado de M√°quina, e isso tamb√©m se aplica aos algoritmos que lidam com dados textuais.\n",
    "\n",
    "‚úçÔ∏è **O pr√©-processamento de texto √© bem diferente do pr√©-processamento num√©rico. As tarefas de pr√©-processamento mais comuns para dados textuais incluem:**\n",
    "\n",
    "* Transformar para letras min√∫sculas\n",
    "* Lidar com n√∫meros, pontua√ß√£o e s√≠mbolos\n",
    "* Divis√£o (splitting)\n",
    "* Tokeniza√ß√£o\n",
    "* Remo√ß√£o de \"stopwords\"\n",
    "* Lematiza√ß√£o\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  <span color=\"green\">Limpeza b√°sica com opera√ß√µes de string nativas do Python</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <p1><span style=\"color: rgba(144, 238, 144, 0.5); \">Quando voc√™ tem um texto n√£o estruturado, j√° pode limp√°-lo com algumas opera√ß√µes de string embutidas no Python.</span></p1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p aling=\">\n",
    "==\n",
    "Limpeza Basica com python\n",
    "==\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Armazenamento das frases em um contexto de Texto no python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    '   Bonjour, comment ca va ?     ',\n",
    "    '    Heyyyyy, how are you doing ?   ',\n",
    "    '        Hallo, wie gehts ?     '\n",
    "]\n",
    "texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Limpando todos o espa√ßos entre as palavras da frase ultilizando `strip`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[text.strip() for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Removendo caracteres do inicil e final de uma string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"abcd Who is abcd ? That's not a real name!!! abcd\"\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.strip('bdac')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Trabalhadando com Replace para substituir palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I love koalas, koalas are the cutest animals on Earth.\"\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.replace(\"koala\", \"panda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  Trabalhando com split para remover um caracter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"linkin park / metallica /red hot chili peppers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.split(\"/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Trabalhando com Lower case para padronizar letras minusculas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"i LOVE football sO mUch. FOOTBALL is my passion. Who else loves fOOtBaLL ?\"\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Trabalhando com Numeros identificando oque s√£o digitos dentro de uma frase\n",
    "  - Agrupamento de texto (text clustering)\n",
    "  - Coleta de palavras-chave (keyphrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"i do not recommend this restaurant, we waited for so long, like 30 minutes, this is ridiculous\"\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text = ''.join(char for char in text if not char.isdigit())\n",
    "cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pontua√ß√µes e simbolos\n",
    "    - A pontua√ß√£o, como \".?!\", e os s√≠mbolos, como \"@#$\", n√£o s√£o √∫teis para a modelagem de t√≥picos.  \n",
    "    - A pontua√ß√£o raramente √© usada corretamente em plataformas de m√≠dia social.  \n",
    "\n",
    ">**Aten√ß√£o:** voc√™ pode querer manter a pontua√ß√£o e os s√≠mbolos se estiver realizando atribui√ß√£o de autoria!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I love bubble tea! OMG so #tasty @channel XOXO @$ ^_^ \"\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string # \"string\" module is already installed with Python\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for punctuation in string.punctuation:\n",
    "    text = text.replace(punctuation, '')\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Combo: strip + lowercase + n√∫meros + pontua√ß√£o/s√≠mbolos \n",
    "    1. `strip()` : Remove espa√ßos em branco no in√≠cio e no final das strings.\n",
    "    2. `lower()` : Converte todo o texto para letras min√∫sculas.\n",
    "    3. **Remover n√∫meros**: Substitui ou elimina n√∫meros usando express√µes regulares.\n",
    "    4. **Remover pontua√ß√£o e s√≠mbolos**: Exclui caracteres como \".?!@#$\" tamb√©m com express√µes regulares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"   I LOVE Pizza 999 @^_^\",\n",
    "    \"  Le Wagon is amazing, take care - 666\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_cleaning(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = ''.join(char for char in sentence if not char.isdigit())\n",
    "\n",
    "    for punctuation in string.punctuation:\n",
    "        sentence = sentence.replace(punctuation, '')\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = [basic_cleaning(sentence) for sentence in sentences]\n",
    "cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  Removendo Tags com RegEx  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "text = \"\"\"<head><body>Hello Le Wagon!</body></head>\"\"\"\n",
    "cleaned_text = re.sub('<[^<]+?>','', text)\n",
    "\n",
    "print (cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "txt = '''\n",
    "    This is a random text, authored by darkvador@gmail.com\n",
    "    and batman@outlook.com, WOW!\n",
    "'''\n",
    "\n",
    "re.findall('[\\w.+-]+@[\\w-]+\\.[\\w.-]+', txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpando com NLTK  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -- O Natural Language Toolkit (NLTK) √© uma biblioteca de PLN que fornece ferramentas de pr√©-processamento e modelagem para dados textuais."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  Documenta√ß√£o de instala√ß√£o\n",
    "*  Site oficial do NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -- Tokeniza√ß√£o  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tokeniza√ß√£o √© basicamente dividir uma senten√ßa, um par√°grafo ou at√© mesmo um texto inteiro em peda√ßos menores, como palavras individuais chamadas tokens.  \n",
    "\n",
    "\"Natural Language Processing\"  \n",
    "‚Üí  \n",
    "`[\"Natural\", \"Language\", \"Processing\"]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- nltk.tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'It is during our darkest moments that we must focus to see the light'\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Make sure the punkt package is properly downloaded and configured\n",
    "nltk.download('punkt', quiet=False)\n",
    "\n",
    "# Set NLTK to use a specific language model - try PY3 as it might be the Python 3 compatible version\n",
    "nltk.data.path.append('/home/codespace/nltk_data')\n",
    "\n",
    "# Explicitly try to use a specific model\n",
    "text = 'It is during our darkest moments that we must focus to see the light'\n",
    "\n",
    "# Alternative approach that doesn't rely on the punkt model\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "word_tokens = tokenizer.tokenize(text)\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -- Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Stopwords** s√£o palavras que s√£o usadas t√£o frequentemente que n√£o carregam muita informa√ß√£o, especialmente para modelagem de t√≥picos\n",
    "- O **NLTK** possui um corpus integrado de stopwords em ingl√™s que pode ser carregado e utilizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# Baixar o recurso de stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Agora tente carregar as stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Exibir algumas stopwords\n",
    "print(list(stop_words)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -- Lemmatizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Screenshot of a comment on a GitHub issue showing an image, added in the Markdown, of an Octocat smiling and raising a tentacle.](image.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Olhe a seguinte frase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'He was RUNNING and EATING at the same time =[. He has a bad habit of swimming after playing 3 hours in the Sun =/'\n",
    "sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Vamos aplicar os seguintes passos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Baixar recursos necess√°rios do NLTK\n",
    "nltk.download('stopwords', download_dir='/home/codespace/nltk_data')\n",
    "nltk.download('wordnet', download_dir='/home/codespace/nltk_data')\n",
    "nltk.download('omw-1.4', download_dir='/home/codespace/nltk_data')  # Open Multilingual WordNet\n",
    "nltk.data.path.append('/home/codespace/nltk_data')\n",
    "\n",
    "def basic_cleaning(text):\n",
    "    # Converter para min√∫sculas\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remover pontua√ß√µes\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Remover n√∫meros\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remover espa√ßos extras\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def simple_tokenize(text):\n",
    "    \"\"\"\n",
    "    Tokeniza um texto dividindo por espa√ßos\n",
    "    \"\"\"\n",
    "    return text.split()\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    \"\"\"\n",
    "    Remove stopwords da lista de tokens\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [word for word in tokens if word not in stop_words]\n",
    "\n",
    "def lemmatize_tokens(tokens):\n",
    "    \"\"\"\n",
    "    Lematiza uma lista de tokens usando WordNetLemmatizer do NLTK\n",
    "    \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "# Exemplo de uso\n",
    "sentence = 'He was RUNNING and EATING at the same time =[. He has a bad habit of swimming after playing 3 hours in the Sun =/'\n",
    "\n",
    "# Etapa 1: Limpeza b√°sica\n",
    "cleaned_sentence = basic_cleaning(sentence)\n",
    "print(\"Ap√≥s limpeza b√°sica:\", cleaned_sentence)\n",
    "\n",
    "# Etapa 2: Tokeniza√ß√£o simples\n",
    "tokens = simple_tokenize(cleaned_sentence)\n",
    "print(\"Ap√≥s tokeniza√ß√£o:\", tokens)\n",
    "\n",
    "# Etapa 3: Remo√ß√£o de stopwords\n",
    "filtered_tokens = remove_stopwords(tokens)\n",
    "print(\"Ap√≥s remo√ß√£o de stopwords:\", filtered_tokens)\n",
    "\n",
    "# Etapa 4: Lematiza√ß√£o\n",
    "lemmatized_tokens = lemmatize_tokens(filtered_tokens)\n",
    "print(\"Ap√≥s lematiza√ß√£o:\", lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercicio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Baixar recursos necess√°rios do NLTK\n",
    "'''\n",
    "nltk.download('stopwords', download_dir='/home/codespace/nltk_data')\n",
    "nltk.download('wordnet', download_dir='/home/codespace/nltk_data')\n",
    "nltk.download('omw-1.4', download_dir='/home/codespace/nltk_data')  # Open Multilingual WordNet\n",
    "nltk.data.path.append('/home/codespace/nltk_data')\n",
    "'''\n",
    "\n",
    "def basic_cleaning(text):\n",
    "    # Converter para min√∫sculas\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remover pontua√ß√µes\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Remover n√∫meros\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remover espa√ßos extras\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def simple_tokenize(text):\n",
    "    \"\"\"\n",
    "    Tokeniza um texto dividindo por espa√ßos\n",
    "    \"\"\"\n",
    "    return text.split()\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    \"\"\"\n",
    "    Remove stopwords da lista de tokens\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [word for word in tokens if word not in stop_words]\n",
    "\n",
    "def lemmatize_tokens(tokens):\n",
    "    \"\"\"\n",
    "    Lematiza uma lista de tokens usando WordNetLemmatizer do NLTK\n",
    "    \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "frases = [\n",
    "    \"The children were playing in the leaves yesterday.\",\n",
    "    \"She studies computer science and is taking three courses.\",\n",
    "    \"The wolves howled at the moon while mice scurried in the grass.\",\n",
    "    \"He was driving faster than the cars around him.\",\n",
    "    \"The chefs used sharp knives to prepare the tastiest dishes.\"\n",
    "]\n",
    "for i in frases:\n",
    "    print(\"==============================================\")\n",
    "    \n",
    "    frase_limpa = basic_cleaning(i)\n",
    "    print (\"frase limpa : \" + frase_limpa)\n",
    "    \n",
    "    frase_token = simple_tokenize(frase_limpa)\n",
    "    print (\"Frase Token : \" , frase_token)\n",
    "\n",
    "    frase_sem_stop = remove_stopwords(frase_token)\n",
    "    print (\"Frase remove stop: \" , frase_sem_stop)\n",
    "\n",
    "    frase_lematizada = lemmatize_tokens(frase_sem_stop)\n",
    "    print (\"frase lematizada: \" , frase_lematizada)\n",
    "\n",
    "    print(\"==============================================\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
