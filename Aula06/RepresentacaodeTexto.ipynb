{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green; text-aling:justify;\">Representação de texto: embeddings e modelos como Word2Vec</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Preparação do Ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importações básicas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as api\n",
    "from sklearn.manifold import TSNE\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download de recursos NLTK (se necessário)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Preparando um Dataset para Treinamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 - Carregando Dados de Exemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dados de exemplo - críticas de filmes (simplificadas)\n",
    "textos = [\n",
    "    \"Este filme é incrível, adorei a atuação do protagonista\",\n",
    "    \"A direção de fotografia é espetacular e o roteiro é envolvente\",\n",
    "    \"Péssimo filme, desperdicei meu tempo assistindo isso\",\n",
    "    \"Os atores são talentosos mas o roteiro é fraco\",\n",
    "    \"Cinematografia belíssima, recomendo assistir no cinema\",\n",
    "    \"Não gostei da história, personagens mal desenvolvidos\",\n",
    "    \"A trilha sonora combina perfeitamente com as cenas\",\n",
    "    \"Filme entediante, previsível do início ao fim\",\n",
    "    \"Os efeitos especiais são impressionantes, tecnologia de ponta\",\n",
    "    \"História emocionante, chorei no final do filme\"\n",
    "]\n",
    "\n",
    "# Verificando os dados\n",
    "for i, texto in enumerate(textos[:3]):  # Mostrando apenas os 3 primeiros\n",
    "    print(f\"Texto {i+1}: {texto}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 - Pré-processamento do Texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def preprocessar_texto(texto):\n",
    "    # Converter para minúsculas\n",
    "    texto = texto.lower()\n",
    "\n",
    "    # Remover caracteres especiais e números\n",
    "    texto = re.sub(r'[^a-záàâãéèêíïóôõöúçñ ]', '', texto)\n",
    "\n",
    "    # Tokenizar\n",
    "    tokens = word_tokenize(texto)\n",
    "\n",
    "    # Remover stopwords (opcional, dependendo da aplicação)\n",
    "    stop_words = set(stopwords.words('portuguese'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# Aplicar pré-processamento a todos os textos\n",
    "textos_preprocessados = [preprocessar_texto(texto) for texto in textos]\n",
    "\n",
    "# Verificar resultado\n",
    "print(\"Exemplo de texto original:\")\n",
    "print(textos[0])\n",
    "print(\"\\nDepois do pré-processamento:\")\n",
    "print(textos_preprocessados[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Implementando Word2Vec com Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 -  Treinando um Modelo do Zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir parâmetros do modelo\n",
    "vector_size = 100    # Dimensionalidade dos vetores\n",
    "window = 5           # Tamanho da janela de contexto\n",
    "min_count = 1        # Frequência mínima das palavras\n",
    "workers = 4          # Número de threads para treinamento\n",
    "sg = 1               # Modelo Skip-gram (1) ou CBOW (0)\n",
    "\n",
    "# Treinar o modelo\n",
    "model = Word2Vec(\n",
    "    sentences=textos_preprocessados,\n",
    "    vector_size=vector_size,\n",
    "    window=window,\n",
    "    min_count=min_count,\n",
    "    workers=workers,\n",
    "    sg=sg\n",
    ")\n",
    "\n",
    "print(f\"Modelo treinado com {len(model.wv.key_to_index)} palavras no vocabulário\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 - Explorando o Modelo Treinado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listar algumas palavras do vocabulário\n",
    "palavras = list(model.wv.key_to_index.keys())\n",
    "print(\"Algumas palavras do vocabulário:\")\n",
    "print(palavras[:10])  # Primeiras 10 palavras\n",
    "\n",
    "# Verificar o vetor de uma palavra específica\n",
    "if 'filme' in model.wv:\n",
    "    vetor_filme = model.wv['filme']\n",
    "    print(f\"\\nVetor da palavra 'filme' (primeiras 10 dimensões):\")\n",
    "    print(vetor_filme[:10])\n",
    "    print(f\"Dimensionalidade do vetor: {len(vetor_filme)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 - Palavras Mais Similares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encontrar palavras mais similares a 'filme'\n",
    "if 'filme' in model.wv:\n",
    "    similares = model.wv.most_similar('filme', topn=5)\n",
    "    print(\"\\nPalavras mais similares a 'filme':\")\n",
    "    for palavra, similaridade in similares:\n",
    "        print(f\"{palavra}: {similaridade:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 - Salvando e Carregando o Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar o modelo\n",
    "model.save(\"word2vec_filmes.model\")\n",
    "\n",
    "# Carregando o modelo salvo\n",
    "modelo_carregado = Word2Vec.load(\"word2vec_filmes.model\")\n",
    "print(\"Modelo carregado com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - Usando Modelos Pré-treinados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listar modelos disponíveis\n",
    "print(\"Alguns modelos disponíveis no gensim-data:\")\n",
    "print([nome for nome in list(api.info()['models'].keys())[:10]])\n",
    "\n",
    "# Carregar um modelo pré-treinado (word2vec-google-news-300)\n",
    "# Nota: Isso pode demorar um pouco na primeira execução (download)\n",
    "try:\n",
    "    # Para português, você pode tentar 'word2vec-portuguese' se disponível\n",
    "    # ou usar modelos do NILC: http://nilc.icmc.usp.br/embeddings\n",
    "    modelo_pretreino = api.load(\"glove-wiki-gigaword-100\")  # Mais rápido que word2vec-google-news-300\n",
    "    print(f\"Modelo pré-treinado carregado com {len(modelo_pretreino.key_to_index)} palavras\")\n",
    "\n",
    "    # Verificar palavras similares usando modelo pré-treinado\n",
    "    if 'computer' in modelo_pretreino:\n",
    "        similares = modelo_pretreino.most_similar('computer', topn=5)\n",
    "        print(\"\\nPalavras mais similares a 'computer':\")\n",
    "        for palavra, similaridade in similares:\n",
    "            print(f\"{palavra}: {similaridade:.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao carregar modelo pré-treinado: {e}\")\n",
    "    print(\"Tente outro modelo ou prossiga sem esta parte\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 - Operações Vetoriais e Analogias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usando modelo pré-treinado para analogias (se disponível)\n",
    "try:\n",
    "    # Famosa analogia: rei - homem + mulher = rainha\n",
    "    if all(word in modelo_pretreino for word in ['king', 'man', 'woman']):\n",
    "        resultado = modelo_pretreino.most_similar(\n",
    "            positive=['king', 'woman'],\n",
    "            negative=['man'],\n",
    "            topn=3\n",
    "        )\n",
    "        print(\"\\nAnalogia: rei - homem + mulher =\")\n",
    "        for palavra, similaridade in resultado:\n",
    "            print(f\"{palavra}: {similaridade:.4f}\")\n",
    "except:\n",
    "    print(\"Não foi possível realizar a operação de analogia com o modelo disponível\")\n",
    "\n",
    "# Podemos tentar outras analogias com nosso modelo treinado\n",
    "# Por exemplo: bom - positivo + negativo = ruim\n",
    "# (Dependendo do tamanho do corpus, pode não funcionar bem para modelos pequenos)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 -  Calculando Similaridade entre Palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para calcular similaridade entre pares de palavras\n",
    "def calcular_similaridade(modelo, pares_palavras):\n",
    "    resultados = []\n",
    "    for par in pares_palavras:\n",
    "        palavra1, palavra2 = par\n",
    "        if palavra1 in modelo.wv and palavra2 in modelo.wv:\n",
    "            similaridade = modelo.wv.similarity(palavra1, palavra2)\n",
    "            resultados.append((par, similaridade))\n",
    "        else:\n",
    "            resultados.append((par, \"Uma ou ambas as palavras não estão no vocabulário\"))\n",
    "    return resultados\n",
    "\n",
    "# Pares de palavras para testar\n",
    "pares = [\n",
    "    ('filme', 'cinema'),\n",
    "    ('bom', 'ruim'),\n",
    "    ('ator', 'atuação'),\n",
    "    ('filme', 'protagonista')\n",
    "]\n",
    "\n",
    "# Calcular similaridades\n",
    "similaridades = calcular_similaridade(model, pares)\n",
    "\n",
    "# Exibir resultados\n",
    "print(\"\\nSimilaridade entre pares de palavras:\")\n",
    "for (palavra1, palavra2), similaridade in similaridades:\n",
    "    if isinstance(similaridade, float):\n",
    "        print(f\"{palavra1} - {palavra2}: {similaridade:.4f}\")\n",
    "    else:\n",
    "        print(f\"{palavra1} - {palavra2}: {similaridade}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7 - Visualização de Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizar_embeddings(modelo, palavras=None, n_palavras=50):\n",
    "    \"\"\"\n",
    "    Visualiza os embeddings em um espaço 2D usando t-SNE.\n",
    "    Args:\n",
    "        modelo: Modelo Word2Vec\n",
    "        palavras: Lista de palavras específicas para visualizar (opcional)\n",
    "        n_palavras: Número de palavras mais frequentes a visualizar (se palavras=None)\n",
    "    \"\"\"\n",
    "    # Obter palavras e vetores\n",
    "    if palavras is None:\n",
    "        palavras = list(modelo.wv.key_to_index.keys())[:n_palavras]\n",
    "    else:\n",
    "        # Filtrar palavras que não estão no vocabulário\n",
    "        palavras = [p for p in palavras if p in modelo.wv]\n",
    "    \n",
    "    if not palavras:\n",
    "        print(\"Nenhuma palavra válida para visualização\")\n",
    "        return\n",
    "    \n",
    "    # Obter vetores e converter para array NumPy\n",
    "    vetores = np.array([modelo.wv[palavra] for palavra in palavras])\n",
    "    \n",
    "    # Reduzir dimensionalidade com t-SNE\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(palavras)-1))\n",
    "    vetores_2d = tsne.fit_transform(vetores)\n",
    "    \n",
    "    # Criar gráfico\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Plotar pontos\n",
    "    x = vetores_2d[:, 0]\n",
    "    y = vetores_2d[:, 1]\n",
    "    plt.scatter(x, y, c='blue', alpha=0.7)\n",
    "    \n",
    "    # Adicionar rótulos (palavras)\n",
    "    for i, palavra in enumerate(palavras):\n",
    "        plt.annotate(\n",
    "            palavra,\n",
    "            xy=(x[i], y[i]),\n",
    "            xytext=(5, 2),\n",
    "            textcoords='offset points',\n",
    "            fontsize=10\n",
    "        )\n",
    "    \n",
    "    plt.title(\"Visualização t-SNE dos Word Embeddings\")\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8 - Aplicação Prática: Classificação de Sentimentos com Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Dados rotulados para exemplo\n",
    "textos_rotulados = textos  # Usando os mesmos textos de antes\n",
    "sentimentos = [1, 1, 0, 0, 1, 0, 1, 0, 1, 1]  # 1: positivo, 0: negativo\n",
    "\n",
    "# Função para gerar vetores de documento usando embeddings\n",
    "def texto_para_vetor(texto, modelo):\n",
    "    \"\"\"Converte um texto em um vetor médio dos embeddings de suas palavras\"\"\"\n",
    "    palavras = preprocessar_texto(texto)\n",
    "    # Filtrar palavras que estão no vocabulário do modelo\n",
    "    palavras_no_vocab = [p for p in palavras if p in modelo.wv]\n",
    "    if not palavras_no_vocab:\n",
    "        # Se nenhuma palavra estiver no vocabulário, retorna vetor de zeros\n",
    "        return np.zeros(modelo.vector_size)\n",
    "    # Calcular a média dos vetores das palavras\n",
    "    vetores = [modelo.wv[palavra] for palavra in palavras_no_vocab]\n",
    "    return np.mean(vetores, axis=0)\n",
    "\n",
    "# Dividir dados em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    textos_rotulados, sentimentos, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# 1. Abordagem com TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=100)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "clf_tfidf = LogisticRegression(random_state=42)\n",
    "clf_tfidf.fit(X_train_tfidf, y_train)\n",
    "y_pred_tfidf = clf_tfidf.predict(X_test_tfidf)\n",
    "\n",
    "# 2. Abordagem com Word Embeddings\n",
    "X_train_emb = np.array([texto_para_vetor(texto, model) for texto in X_train])\n",
    "X_test_emb = np.array([texto_para_vetor(texto, model) for texto in X_test])\n",
    "\n",
    "clf_emb = LogisticRegression(random_state=42)\n",
    "clf_emb.fit(X_train_emb, y_train)\n",
    "y_pred_emb = clf_emb.predict(X_test_emb)\n",
    "\n",
    "# Comparar resultados\n",
    "print(\"\\nResultados com TF-IDF:\")\n",
    "print(classification_report(y_test, y_pred_tfidf))\n",
    "\n",
    "print(\"\\nResultados com Word Embeddings:\")\n",
    "print(classification_report(y_test, y_pred_emb))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 9 - Word Embeddings Contextuais com spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Carregar modelo spaCy (pequeno)\n",
    "try:\n",
    "    nlp = spacy.load(\"pt_core_news_sm\")  # Para português\n",
    "    # Alternativa para inglês: nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    # Exemplo de texto\n",
    "    texto = \"O banco está cheio de dinheiro. Eu sentei no banco da praça.\"\n",
    "\n",
    "    # Processar o texto\n",
    "    doc = nlp(texto)\n",
    "\n",
    "    # Examinar embeddings para cada ocorrência da palavra \"banco\"\n",
    "    for token in doc:\n",
    "        if token.text.lower() == \"banco\":\n",
    "            contexto = doc[max(0, token.i-3):min(len(doc), token.i+4)]\n",
    "            print(f\"Contexto: {contexto}\")\n",
    "            print(f\"Vetor (10 primeiras dimensões): {token.vector[:10]}\")\n",
    "            print(f\"Dimensão do vetor: {len(token.vector)}\")\n",
    "            print(\"-\" * 50)\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao carregar spaCy: {e}\")\n",
    "    print(\"Talvez seja necessário instalar os modelos com:\")\n",
    "    print(\"python -m spacy download pt_core_news_sm\")\n",
    "    print(\"ou\")\n",
    "    print(\"python -m spacy download en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10 - FastText: Embeddings com Subpalavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "# Treinar modelo FastText\n",
    "modelo_fasttext = FastText(\n",
    "    sentences=textos_preprocessados,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=1,\n",
    "    workers=4,\n",
    "    sg=1\n",
    ")\n",
    "\n",
    "# Testar com palavras que não estão no corpus\n",
    "palavras_teste = [\"filmagem\", \"cinematográfico\", \"atuações\"]\n",
    "\n",
    "print(\"\\nEmbeddings para palavras fora do vocabulário (FastText):\")\n",
    "for palavra in palavras_teste:\n",
    "    # Verificar se a palavra está no vocabulário original\n",
    "    no_vocab = palavra in model.wv\n",
    "    # Obter vetor do FastText (funciona mesmo para palavras fora do vocabulário)\n",
    "    vetor = modelo_fasttext.wv[palavra]\n",
    "    print(f\"'{palavra}' (no vocabulário: {no_vocab})\")\n",
    "    print(f\"Primeiras 5 dimensões do vetor: {vetor[:5]}\")\n",
    "\n",
    "    # Encontrar palavras similares\n",
    "    similares = modelo_fasttext.wv.most_similar(palavra, topn=3)\n",
    "    print(f\"Palavras similares a '{palavra}':\")\n",
    "    for p, sim in similares:\n",
    "        print(f\"  {p}: {sim:.4f}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11 - . Exercícios Práticos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'spacy' has no attribute 'wv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 34\u001b[0m\n\u001b[1;32m     24\u001b[0m documentos \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     25\u001b[0m     documento1,\n\u001b[1;32m     26\u001b[0m     documento2,\n\u001b[1;32m     27\u001b[0m     documento3\n\u001b[1;32m     28\u001b[0m ]\n\u001b[1;32m     31\u001b[0m docsProcessados \u001b[38;5;241m=\u001b[39m [preprocessar_texto(doc) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documentos]\n\u001b[1;32m     33\u001b[0m similaridadeDocs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m---> 34\u001b[0m     \u001b[43msimilaridade_documentos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdoc2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspacy\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m doci, doc1 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(docsProcessados)  \n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m docj, doc2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(docsProcessados) \u001b[38;5;28;01mif\u001b[39;00m doci \u001b[38;5;241m<\u001b[39m docj\n\u001b[1;32m     37\u001b[0m ]\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(docsProcessados)\n",
      "Cell \u001b[0;32mIn[29], line 5\u001b[0m, in \u001b[0;36msimilaridade_documentos\u001b[0;34m(doc1, doc2, modelo)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Calcula a similaridade entre dois documentos usando embeddings\"\"\"\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Iterando sobre as partes de doc1 e doc2 (como frases ou palavras)\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m vetores_doc1 \u001b[38;5;241m=\u001b[39m [\u001b[43mtexto_para_vetor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparte\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodelo\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m parte \u001b[38;5;129;01min\u001b[39;00m doc1]\n\u001b[1;32m      6\u001b[0m vetores_doc2 \u001b[38;5;241m=\u001b[39m [texto_para_vetor(parte, modelo) \u001b[38;5;28;01mfor\u001b[39;00m parte \u001b[38;5;129;01min\u001b[39;00m doc2]\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Calcular a similaridade média entre todos os pares de vetores\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[20], line 15\u001b[0m, in \u001b[0;36mtexto_para_vetor\u001b[0;34m(texto, modelo)\u001b[0m\n\u001b[1;32m     13\u001b[0m palavras \u001b[38;5;241m=\u001b[39m preprocessar_texto(texto)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Filtrar palavras que estão no vocabulário do modelo\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m palavras_no_vocab \u001b[38;5;241m=\u001b[39m [p \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m palavras \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmodelo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwv\u001b[49m]\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m palavras_no_vocab:\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# Se nenhuma palavra estiver no vocabulário, retorna vetor de zeros\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mzeros(modelo\u001b[38;5;241m.\u001b[39mvector_size)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'spacy' has no attribute 'wv'"
     ]
    }
   ],
   "source": [
    "def similaridade_documentos(doc1, doc2, modelo):\n",
    "    \"\"\"Calcula a similaridade entre dois documentos usando embeddings\"\"\"\n",
    "    vetores_doc1 = [texto_para_vetor(parte, modelo) for parte in doc1]\n",
    "    vetores_doc2 = [texto_para_vetor(parte, modelo) for parte in doc2]\n",
    "\n",
    "    similaridades = []\n",
    "    for vetor1 in vetores_doc1:\n",
    "        for vetor2 in vetores_doc2:\n",
    "            similaridade = np.dot(vetor1, vetor2) / (np.linalg.norm(vetor1) * np.linalg.norm(vetor2))\n",
    "            similaridades.append(similaridade)\n",
    "    \n",
    "    return np.mean(similaridades)\n",
    "\n",
    "# Exercício: Calcule a similaridade entre os documentos abaixo\n",
    "documento1 = \"O filme tem uma história envolvente e atuações convincentes\"\n",
    "documento2 = \"A narrativa do filme é cativante e os atores são excelentes\"\n",
    "documento3 = \"O restaurante tem comida deliciosa e preços acessíveis\"\n",
    "\n",
    "# Calcular similaridades (implemente sua solução)\n",
    "documentos = [\n",
    "    documento1,\n",
    "    documento2,\n",
    "    documento3\n",
    "]\n",
    "\n",
    "\n",
    "docsProcessados = [preprocessar_texto(doc) for doc in documentos]\n",
    "\n",
    "similaridadeDocs = [\n",
    "    similaridade_documentos(doc1, doc2, spacy)  \n",
    "    for doci, doc1 in enumerate(docsProcessados)  \n",
    "    for docj, doc2 in enumerate(docsProcessados) if doci < docj\n",
    "]\n",
    "\n",
    "print(docsProcessados)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
